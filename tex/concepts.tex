This chapter introduces the fundamental concepts related to the developments presented in this thesis. The major areas introduced are: Epigenetics, Genetics, (Epi)Genomic data analysis.


\section{Epigenetics alterations}
\subsection{DNA methylation}
\section{Histone modifications}

\section{Genetics alterations}


\section{(Epi)Genomic data analysis}

\subsection{Statistical analysis}

\subsubsection{Hypothesis Testing}

Human genetic studies aims to identify if a phenotype is related to the genotypes
at various loci, that is, if genetic variations have an influence on risk of
disease or other health-related phenotypes.
Statistical analysis is a crucial to present the findings in an
interpretable and objective manner \cite{sham2014statistical}.

The most popular hypothesis testing approach used to test if
genotypes and phenotypes are related  is the frequentist significance testing approach.
This is a classical approach that involves setting up two competing hypothesis: a
null hypothesis ($H_0$\simbolo{H_0}{Null hypothesis}) and an alternative hypothesis ($H_1$\simbolo{H_1}{Alternative hypothesis}).

Computing the statistical significance can be done using a one-tailed or a two-tailed test.
A two-sided test is appropriate to evaluate both direction of the test, for example,
is the estimated value smaller or higher than the reference, which acctualy test if the
 estimated value is different from the reference.
A one-sided test is  is appropriate to evaluate only one direction of the test,
for example, is the estimated v alue smaller than the reference.
An example in genetic studies for a two-sided test would be $H_0$
hypothesis that genotypes has no effect on the phenotypes
while the $H_1$ hypothesis is that there is a effect.
Table \ref{hypothesis-tests} shows other examples.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h!]
\centering
\caption[Hypothesis tests]{Example of three hypothesis tests about the population mean $\mu$. In genetics it could be the mean level of expression of a gene.}
\label{hypothesis-tests}
\begin{tabular}{@{}lll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Type}} & \multicolumn{1}{c}{\textbf{Null}} & \multicolumn{1}{c}{\textbf{Alternative}} \\ \midrule
Right-tailed & $H_{0}:\mu = 0$ & $H_{1}: \mu >  0 $  \\
Left-tailed & $H_{0}:\mu = 0$ & $H_{1}: \mu <  0 $  \\
Two-tailed & $H_{0}:\mu = 0$ & $H_{1}: \mu \neq 0 $ \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Making a decision: P-value approach}

The decision to reject or accept $H_0$ is made based on the calculation of a test statistic (T) from the observed data.
As the value of T depends on particular individuals in the population, repeating the study
using  different random samples from the population would provide of many different values for T.
These set of T can be summarized as a probability distribution.

% Errors in hypothesis testing
Even though, the decision made to reject or accept $H_0$ just state that we had
enough evidence to behave one way or the other.
The rejection of the null hypothesis does not prove that the alternative hypothesis is true as
the acceptance the null hypothesis does not prove that the null hypothesis is true.
It might happen that null hypothesis was reject when it was true, or it was not
rejected when it was false. The first error in statistics is called a Type I error ("false positive"),
 while the second is called a Type II error ("false negative").
Table \ref{type_errors} shows the relations between truth/falseness of the null hypothesis and outcomes of the test.

It is denotated  rate of the type I error  or significance level
$\alpha$\simbolo{\alpha}{Significance level} the probability of having a false positive.
Normally, the significance level is set to 5\%, implying that it is acceptable to have a 5\%
probability of incorrectly rejecting the null hypothesis. With the same logic, the rate of the
type II error is denoted by $\beta$\simbolo{\beta}{rate of the type II error}.

% Please add the following required packages to your document preamble:
\begin{table}[]
  \centering
  \caption{Type I and II Errors. $\alpha = P(\textrm{Type I Error)}$, $\beta = P(\textrm{Type II Error})$}
  \label{type_errors}
  \begin{tabular}{ccc}
    \toprule
    \textbf{Decision} & \textbf{$H_0$ is True} & \textbf{$H_0$ is False} \\ \midrule
  Do Not Reject $H_0$ & Correct Decision  & Incorrect Decision (1 - $\beta$)\\
  Rejct $H_0$ & Incorrect Decision (1 -  $\alpha$)& Correct Decision \\ \bottomrule
  \end{tabular}
\end{table}

To make a decision wheter to reject or accept the null hypothesis, the concept of
probability value was introduced.
\citeonline{wasserstein2016asa} defined a p-value  ($\textrm{p-value}\in [0,1]$) as the probability under a specified statistical model, constructed under a set of assumptions (normally “null hypothesis"), that a statistical summary of the data
(e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value \cite{wasserstein2016asa}. That means, the smaller the p-value, the greater the statistical incompatibility of the data with the null hypothesis and greater the p-value more compatibible is the data with the null hypothesis.
In summary, if P-value is small (e.g.$\textrm{P-value} \leq \alpha$) then the null hypothesis is rejected,
otherwise it is not rejected.

It is important to highlight that a p-value does not measure the size of an effect or the importance of a result.
It might happen that a very small effect produces smaller p-values if the
sample size is big or measurement precision is high.
On the other hand, a large effect might produce higher p-values if
the sample size is small or measurements are imprecise.


%The P-value approach consists in the following steps to conducting any hypothesis test:
%\begin{enumerate}
%  \item Set $H_0$ (null hypotheses) and $H_1$ (alternative hypotheses)
%  \item Using the sample data and assuming the null hypothesis is true, calculate the value of the test statistic.
%  Again, to conduct the hypothesis test for the population mean $\mu$, we use the t-statistic $t^{\ast}= \frac{\bar{x}-\mu}{s/\sqrt{n}}$ which follows a t-distribution with n - 1 degrees of freedom.
%  \item Using the known distribution of the test statistic, calculate the P-value: "If the null hypothesis is true, what is the probability that we'd observe a more extreme test statistic in the direction of the alternative hypothesis than we did?"
%  \item Set the significance level, $\alpha$, the probability of making a Type I
%  error to be small - 0.01, 0.05, or 0.10. Compare the P-value to  $\alpha$.
%  If the P-value is less than (or equal to)  $\alpha$, reject the null hypothesis
%  in favor of the alternative hypothesis. If the P-value is greater than  $\alpha$,
%  do not reject the null hypothesis.
%\end{enumerate}


%For a one-sided test (for example, a test for effect size greater than zero), the definition of the P value is slightly more complicated: P* = P/2 if the observed effect
%is in the pre-specified direction, or P* = (1 – P)/2 otherwise, where P is defined as above. In the Neyman–Pearson hypothesis testing framework, if the P value is smaller than a preset threshold α (for example, 5 × 10−8 for genome-wide association studies), then H is rejected and the result is considered to be significant.

%By setting up a hypothesis test in this manner, the probability of making the error of
% rejecting H0 when it is true (that is, a type 1 error) is ensured to be α. However, another possible type of error is the failure to reject H0 when it is false (that is, type 2 error, the probability of which is denoted as β). Statistical power is defined as 1 – β (that is,
%the probability of correctly rejecting H0 when a true association is present).


\subsubsection{Correcting for multiple testing}

When performing  a set of statistical inferences simultaneously more likely erroneous inferences are to occur.
For example, if 100 tests are carried
out, then 5\% of them (that is 5 tests) are expected to
have $P-value < 0.05$ by chance when $H_0$ is in fact true for all the tests.
Compared to a single test (equations \ref{eq_error} and \ref{eq_error2}), the probability of having a type 1 error multiple test is given by the equations \ref{eq_multiple_error} and \ref{eq_multiple_errorb} \cite{vsidak1967rectangular}.

\begin{subequations}

\begin{align}
  P(\textrm{Making an error)} = \alpha \label{eq_error}\\
  P(\textrm{Not making an error)} = 1 - \alpha \label{eq_error2}\\
  P(\textrm{Not making an error in m tests)} = (1 - \alpha)^m  \label{eq_multiple_error}\\
  P(\textrm{Making at least 1 error in m tests}) = 1 - (1 - \alpha)^m  \label{eq_multiple_errorb}
\end{align}
\end{subequations}

To handle this multiple statistical testing problem,
some techniques  to re-calculating probabilities obtained from a statistical test which was repeated multiple times
have been developed to prevent the inflation of false positive rates.


Among the different approaches to control type I errors we have
\sigla{FWER}{Family-wise error rate} which controls the probability of at least one type I error,
and \sigla{FDR}{False discovery rate}
which controls the expected proportion of Type I errors
among the rejected hypotheses. Compapred to FDR, controlling FWER is extremely conservative
approach as the power to detect $H_1$ gets very small.

Among the different adjustment methods to control FWER includes the Bonferroni correction
  in which the p-values are multiplied by the number of comparisons ($M * P_i < \alpha$) and the Holm correction, $P-adjusted_i = P_i * (M + 1 - i)$, where $i \in \{1,2,\dots,n\}$ and smaller the p-value is smaller will the index $i$ be \cite{aickin1996adjusting}.
 The Benjamini-Hochberg (BH) method to control FDR procedure will identify the largest $k$,
 such that $P_k \leq \frac{k}{m}\alpha$, all null hypotheses $H_i$ for $i \in \{1,\ldots,k\}$ are rejected.


These methods makes the assumption that
the tests are independent tests, which often is not valid for genomics data.
For dependent tests, permutation methods are often used to calculate
 p-values.  This approach recalculate a p-value comparing the P-value calculated
 from the real data test with random ones,
 which are performed by randomly shuffling the case–control (or phenotype)
 labels. All $M$ tests are recalculated on the reshuffled data set, with the smallest P value of these M tests being recorded. The procedure is repeated for many times to construct an empirical frequency distribution of the smallest P values.
This  empirical adjusted P value ($P_{*}$) is given by: $$P_{*} = \frac{r + 1}{n + 1}$$ where $n$ are the number of
permutation carried out, and $r$ is the number of permutated p-values smaller than P-value calculated
from the real data.

For example, considering $\textrm{P-value = 0.1}$ and the
permutated p-values $$P_{permu} =\{0.001,0.01,0.02,0.03,0.05,0.2,0.5,0.6,1\}$$ the first 5 permutated p-values
are smaller than the original p-value, which would give us $r = 5$, resulting in:
$$P_{*} = \frac{r + 1}{n + 1} =  \frac{5 + 1}{9 + 1} = 0.6 $$
It is important to highlight that
a high number of permutations is required in order to produce reliable permutated p-value adjusted.
 \cite{davison1997bootstrap,north2002note,north2003note,sham2014statistical}.


\subsubsection{Nonparametric and parametric tests}

Statistical procedures can be classified into two groups:  Parametric and nonparametric.
Parametric statistical procedures rely on assumptions about the shape of the distribution
(i.e., assume a normal distribution) in the underlying population and about the form or
parameters (i.e., means and standard deviations) of the assumed distribution.
While nonparametric statistical procedures doest not rely or rely on only few assumptions about the shape or
parameters of the population distribution from which the sample was drawn.
Some of these producedures are summarized in Table \ref{Parametric-nonparametric}.

The t-test, a parametric test, is the most widely used statistical test for comparing the means of two independent groups.
It assumes that the data are distributed Normally, that samples from different groups are independent and that the variances between the groups are equal. The most commonly used nonparametric test in this situation is the \sigla{WRST}{Wilcoxon Rank Sum Test} and the closely related \sigla{MWU}{Mann-Whitney U-test}. The WRST assumes that observations from the different groups are random samples (i.e. independent and identically distributed) from their respective populations and are mutually independent and that the observations are ordinal or continuous measurements.
When there are more than two groups being compared,
the nonparametric test used is \sigla{KW}{Kruskal-Wallis test}, a generalization of the WRST. KW is the nonparametric equivalent to \sigla{ANOVA}{Analysis of variance}.


\bgroup
\def\arraystretch{2.0}%  1 is the default, change whatever you need

\begin{table}[h!]
\footnotesize
\centering
\caption{Summary of parametric and non parametric procedures.}
\label{Parametric-nonparametric}
\begin{tabular}{p{3.5cm}p{4cm}p{3cm}p{3cm}}
\toprule
\textbf{Analysis Type} & \textbf{Example} & \textbf{Parametric} & \textbf{Nonparametric} \\ \midrule
Compare means between two distinct/independent groups & Is the mean TP53 gene expression for control group different from the mean for treatment group? & Two-sample t-test & Wilcoxon ranksum test \\
Compare two quantitative measurements taken from the same individual & Was there a change in gene expression after the treatment? & Paired t-test & Wilcoxon signedrank test \\
Compare means between three or more distinct/independent groups & For a given three groups (e.g., placebo, drug \#1, drug \#2), is the TP53 gene expression different among the three groups? & Analysis of variance (ANOVA) & Kruskal-Wallis test \\
Estimate the degree of association between two quantitative variables & Is age related to the TP53 gene expression? & Pearson coefficient of correlation & Spearman’s rank correlation \\ \bottomrule
\end{tabular}
\end{table}
\egroup

% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2743502/
% http://blog.minitab.com/blog/adventures-in-statistics-2/choosing-between-a-nonparametric-test-and-a-parametric-test


\subsection{Survival analysis}

In the medical sciences, in several situations it is necessary to evaluate if
a treatment had a beneficial effect on the survival of patients.
For this, it is measured the fraction of patients living for a certain amount of time after treatment.

It is called "Survival times" data that measure follow-up time from a
defined starting time to the occurrence of a given event
(e.g. from the diagnosis of a disease to death) \cite{bewick2004statistics}.
This survival times are "censored" when there is a follow-up time
but the event has not yet occurred or is not known to have occurred.
That might happen if a patient drops out of the study before its end,
or if you are studying a treatment and it is not over yet.

%Standard statistical techniques cannot usually be applied
%because the underlying distribution is rarely Normal and the
%data are often ‘censored’.
\subsubsection{Kaplan–Meier method: Estimating the survival curve}

It is defined  survival function $S(t)$ is defined as the
probability of surviving at least to time t, while a
 graph of S(t) against t is called the survival curve.
To estimate this curve there exists the Kaplan-Meier method:
$$ S(t) = \prod_{i: t_i\leq t}\left(1 - \frac{d_i}{n_i}\right),$$
\simbolo{S(t)}{Kaplan–Meier estimator} where $d_{i}$ are the number of events and  $n_{i}$
the total individuals at risk at time $i$. Table \ref{survival-example} shows
an example of survival data and Table \ref{Kaplan-Meier-example} shows
and example of applying Kaplan-Meier method. It is important to highlight that
for a censored time the proportion surviving will be 1, that means
an individual is considered to be at risk of dying in the next event of the censoring
but not in subsequent events \cite{bland2004logrank}.

\begin{table}[h!]
\centering
\caption[Survival time example]{Survival time and status for a group of patients.}
\label{survival-example}
\begin{tabular}{ccc}
\hline
\multicolumn{1}{l}{\textbf{Patient ID}} & \multicolumn{1}{l}{\textbf{Survival times (in days)}} & \multicolumn{1}{l}{\textbf{Status}} \\ \hline
1 & 1 & Dead \\
2 & 1 & Dead \\
3 & 2 & Alive (cencored) \\
4 & 3 & Dead \\ \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption[Kaplan-Meier method  example]{Kaplan-Meier method example for table \ref{survival-example}}
\label{Kaplan-Meier-example}
\begin{tabular}{cp{3cm}p{3cm}p{3cm}c}
\hline
\textbf{Interval} & \textbf{$n_i$: patient at risk at time $t_i^-$} &
\textbf{$d_i$ = deaths at time $t_i$} &
\textbf{$c_i$ = censored at time $t_i$} &  \textbf{$S(t)$}  \\ \hline
$[0,1)$ & 4          & 0 & 0 & 1 \\
$[1,3)$ & 4 - 0 = 4  & 2 & 1 & $1 - \frac{2}{4} = 0.5$ \\
$[3,\textrm{End of study}]$  & 4 - 2 - 1 = 1  & 1 & 0 & $0.5 * (1 - \frac{1}{1})$ = 0 \\ \hline
\end{tabular}
\end{table}


\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{images/example_survival.pdf}
\caption[Example of survival curve.]{
Example of survival curve for table \ref{survival-example}. Censored data is marked (+) in the plot.}
\end{figure*}



%In analyzing survival data, two functions that are dependent
%on time are of particular interest: the survival function and the
%hazard function.
%The survival function S(t) is defined as the
%probability of surviving at least to time t. The hazard function
%h(t) is the conditional probability of dying at time t having
%survived to that time.

\subsubsection{Comparing survival curves of two groups using the log rank test}

 To compare the survival distributions of two or more groups,  the hypothesis test log-rank test is
 used to test the null hypothesis that there is no difference between the populations
 in the probability of an event at any time point.
%
 The approximated statistics used for comparison purposes for $k$ groups is
 $$T = \sum_{i \in \{1,\ldots,k\}}\frac{(O_i - E_i)^2}{E_i},$$
 where $O_i$ is the observed numbers of death in  group $i$
 while $E_i$ is its expected numbers of deaths.
 If the null hypothesis is true, $T$ is distributed approximately as a $\chi^2_{k-1}$ \cite{matthews1996using}.
If T calculated is $9.44$, and $k = 2$,  evaluating the quantile function (also known as “inverse CDF” or “ICDF”) of the chi-squared distribution the significance level of these data is equal to
$P_r(\chi^2_{1}\geq9.44 = 0.002)$ \cite{yau2012r}.
For a given cut-off, normally $0.05$, the results with p-value smaller are considered significant.

\subsection{Machine Learning}

Machine learning is a field of computer science focused on the development and application of algorithms that improve with experience \cite{mitchell1997machine}.

In genetics and genomics, its  has been applied
 for the interpretation of large genomic data sets and annotation of a wide variety of genomic sequence elements.
For example, for the detection of  transcription start sites (TSSs) locations, which have proven hard to detect in silico due to the complexity and the fairly diffuse structures of Eukaryotic promoters, \citeonline{down2002computational} developped a machine-learning method is able to build useful models of promoters for $>50\%$ of human transcription start sites \cite{down2002computational}.

The machine learning techniques can classified into two main categories: supervised and unsupervised learning (Mitchell, 1997). The supervised learning, which aims to infer data labels by learning from already labeled data, has three stages: design, model and test. The first stage refers to the selection of a learning algorithm used to learn from data (e.g. choose between support vector machines or random forest algoritms) and its training data. The second stage is the creation of a model from labeled data using the algorithm selected priviously.  The last stage uses of this generated model to  preditct the labels of unlabeled data.
The unsupervised learning methods, on the other hand,
cluster the data without using labels, which requires an additional step in which semantics must be manually assigned to each cluster. As this discovery is not tied to previously defined classes, these methods have as benefit the ability to identify potentially novel types of genomic elements.




\subsubsection{Supervised learning}
\subsubsection{Unsupervised learning}



%When a labelled training set is not available, unsupervised learning is required. For example, consider the interpretation of a heterogeneous collection of epigenomic data sets, such as those generated by the Encyclopedia of DNA Elements (ENCODE) Consortium and the Roadmap Epigenomics Project. A priori, we expect that the patterns of chromatin accessibility, histone modifications and transcription factor binding along the genome should be able to provide a detailed picture of the biochemical and functional activity of the genome. We may also expect that these activities could be accurately summarized using a fairly small set of labels. If we are interested in discovering what types of label best explain the data, rather than imposing
%a pre-determined set of labels on the data, then we must use unsupervised rather than supervised learning. In this type of approach, the machine learning algorithm uses only the unlabelled data and the desired number of different labels to assign as input.

% In Transcriptomics analysis clustering is used to build groups of genes with related expression patterns

Some of the data clustering techniques are listed below:

As stated before, unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses.

The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data. These algorithms seeks to group a set of objects into cluster (groups) such that those in the same group are more similar to each other than to those in other cluster.
%In genomics, the Hierarchical clustering, Centroid-based clustering and PCA are commonly used.

Hierarchical clustering:
This a method builds a hierarchy of clusters, through two main approaches,the "bottom up" approach in which  each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy and the  "top down" approach in which all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy. To make a decision which are the clusters being merged or divided a metric to measure of dissimilarity between sets of observations is required. Those metrics can be for example distance such as Euclidean distance, Manhattan distance, etc. For the "bottom up" approach, clusters with smalled distance are merged.
The results of hierarchical clustering are usually presented in a dendrogram, in which the objects are represented in one of the axis while the distance between clusters are represented in the other axis.


Centroid-based clustering:
These set of algorithms identifies objects that cluster together into $k$ clusters. Let $S = \{S_1,S_2,\ldots,S_k\}$  the set of clusters, this algorithms tries to minimize the squared distances of the elements from the cluster center:
$$ J(M) = min\sum_{i = 1}^{k}\sum_{x \in S_i}\Vert x_i - \mu_i\Vert^2,$$
where $x_i$ is a data element, and $\mu_i$ is the center of points of  a $S_i$ cluster it belongs.
The most common algorithm is the k-means, an iterative refinement technique that starts by randomly choosing k observations from the data set and uses these as the initial means. The nearest points are clustered and the mean of the cluster is recalculated. Using the new means, again the cluster are updated. The algorithm has converged when the groups no longer change.



%Another classical and very useful example of unsupervised techniques is the Principal Component Analysis (PCA)
